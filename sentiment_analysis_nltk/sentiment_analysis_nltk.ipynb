{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following tutorial at https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#shows how to do sentiment analysis using the nltk toolkit for python. This was used for the comment dataset for NoSleep\n",
    "#https://www.nltk.org/api/nltk.tokenize.html was also used\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only has to be run once. It downloads the modules that are necessary\n",
    "#to perform sentiment analysis\n",
    "\n",
    "#The punkt module is a pre-trained model that helps you tokenize words and sentences.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>sortKey</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>id</th>\n",
       "      <th>storyId</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bbyzum</td>\n",
       "      <td>1555131600</td>\n",
       "      <td>3</td>\n",
       "      <td>/r/nosleep/comments/bbyzum/i_finally_got_my_re...</td>\n",
       "      <td>t2_154rlg</td>\n",
       "      <td>eknfnl9</td>\n",
       "      <td>bbyzum</td>\n",
       "      <td>Senpee</td>\n",
       "      <td>t1_ekndmde</td>\n",
       "      <td>r/wooooosh</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_ddqdef</td>\n",
       "      <td>1570510800</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/ddqdef/how_to_survive_camp...</td>\n",
       "      <td>t2_4e933umj</td>\n",
       "      <td>f2tug43</td>\n",
       "      <td>ddqdef</td>\n",
       "      <td>SlurpeeSlurper</td>\n",
       "      <td>t1_f2tsmpr</td>\n",
       "      <td>I'd keep away from him for a start. Good luck!</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_enacmh</td>\n",
       "      <td>1578978000</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/enacmh/my_mom_hired_a_clow...</td>\n",
       "      <td>t2_4x1l8o7v</td>\n",
       "      <td>fe2hmu8</td>\n",
       "      <td>enacmh</td>\n",
       "      <td>HesAPhantom304</td>\n",
       "      <td>t1_fe2hb55</td>\n",
       "      <td>Oh. Sorry then. I just got a little confused f...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_aby8yi</td>\n",
       "      <td>1546664400</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/aby8yi/i_woke_up_to_find_a...</td>\n",
       "      <td>t2_2cguwyzj</td>\n",
       "      <td>ed4xavl</td>\n",
       "      <td>aby8yi</td>\n",
       "      <td>PepperidgeFarmMembas</td>\n",
       "      <td>t3_aby8yi</td>\n",
       "      <td>You never opened your mother’s present....what...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_c3kr9t</td>\n",
       "      <td>1561525200</td>\n",
       "      <td>2</td>\n",
       "      <td>/r/nosleep/comments/c3kr9t/i_kill_evil_that_th...</td>\n",
       "      <td>t2_xv4yuqt</td>\n",
       "      <td>erxg37c</td>\n",
       "      <td>c3kr9t</td>\n",
       "      <td>Vaughawa</td>\n",
       "      <td>t3_c3kr9t</td>\n",
       "      <td>Amazing</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>t3_azw37m</td>\n",
       "      <td>1552539600</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/azw37m/a_cat_kept_on_makin...</td>\n",
       "      <td>t2_y42gk</td>\n",
       "      <td>eidcu8a</td>\n",
       "      <td>azw37m</td>\n",
       "      <td>MJGOO</td>\n",
       "      <td>t1_eiayjsx</td>\n",
       "      <td>88 miles an hour!!!</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>t3_e4kgrj</td>\n",
       "      <td>1575349200</td>\n",
       "      <td>22</td>\n",
       "      <td>/r/nosleep/comments/e4kgrj/if_the_time_stops_a...</td>\n",
       "      <td>t2_3z2en751</td>\n",
       "      <td>f9ec3fk</td>\n",
       "      <td>e4kgrj</td>\n",
       "      <td>scps53770</td>\n",
       "      <td>t1_f9ebuvh</td>\n",
       "      <td>What the actual fuck did I just read Lmao!</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>t3_cufufd</td>\n",
       "      <td>1566709200</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/cufufd/i_have_the_perfect_...</td>\n",
       "      <td>t2_22igqeln</td>\n",
       "      <td>exw0qwv</td>\n",
       "      <td>cufufd</td>\n",
       "      <td>bothicc</td>\n",
       "      <td>t1_exv7rvj</td>\n",
       "      <td>Me! Me! I want one!</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>t3_cmykhy</td>\n",
       "      <td>1565326800</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/cmykhy/the_previous_tenant...</td>\n",
       "      <td>t2_102ihk</td>\n",
       "      <td>ew69zua</td>\n",
       "      <td>cmykhy</td>\n",
       "      <td>eggplantsaredope</td>\n",
       "      <td>t3_cmykhy</td>\n",
       "      <td>Really weird that you couldnt bring the garden...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>t3_d1zd4f</td>\n",
       "      <td>1568264400</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/d1zd4f/my_grandpa_a_retire...</td>\n",
       "      <td>t2_9uzm7</td>\n",
       "      <td>ezsrtr6</td>\n",
       "      <td>d1zd4f</td>\n",
       "      <td>No_H_in_Cage</td>\n",
       "      <td>t1_ezsrtjq</td>\n",
       "      <td>There is no 'H' in Nicolas Cage.\\n\\nSource: ht...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      link_id     sortKey  score  \\\n",
       "0   t3_bbyzum  1555131600      3   \n",
       "1   t3_ddqdef  1570510800      1   \n",
       "2   t3_enacmh  1578978000      1   \n",
       "3   t3_aby8yi  1546664400      1   \n",
       "4   t3_c3kr9t  1561525200      2   \n",
       "..        ...         ...    ...   \n",
       "95  t3_azw37m  1552539600      1   \n",
       "96  t3_e4kgrj  1575349200     22   \n",
       "97  t3_cufufd  1566709200      1   \n",
       "98  t3_cmykhy  1565326800      1   \n",
       "99  t3_d1zd4f  1568264400      1   \n",
       "\n",
       "                                            permalink author_fullname  \\\n",
       "0   /r/nosleep/comments/bbyzum/i_finally_got_my_re...       t2_154rlg   \n",
       "1   /r/nosleep/comments/ddqdef/how_to_survive_camp...     t2_4e933umj   \n",
       "2   /r/nosleep/comments/enacmh/my_mom_hired_a_clow...     t2_4x1l8o7v   \n",
       "3   /r/nosleep/comments/aby8yi/i_woke_up_to_find_a...     t2_2cguwyzj   \n",
       "4   /r/nosleep/comments/c3kr9t/i_kill_evil_that_th...      t2_xv4yuqt   \n",
       "..                                                ...             ...   \n",
       "95  /r/nosleep/comments/azw37m/a_cat_kept_on_makin...        t2_y42gk   \n",
       "96  /r/nosleep/comments/e4kgrj/if_the_time_stops_a...     t2_3z2en751   \n",
       "97  /r/nosleep/comments/cufufd/i_have_the_perfect_...     t2_22igqeln   \n",
       "98  /r/nosleep/comments/cmykhy/the_previous_tenant...       t2_102ihk   \n",
       "99  /r/nosleep/comments/d1zd4f/my_grandpa_a_retire...        t2_9uzm7   \n",
       "\n",
       "         id storyId                author   parent_id  \\\n",
       "0   eknfnl9  bbyzum                Senpee  t1_ekndmde   \n",
       "1   f2tug43  ddqdef        SlurpeeSlurper  t1_f2tsmpr   \n",
       "2   fe2hmu8  enacmh        HesAPhantom304  t1_fe2hb55   \n",
       "3   ed4xavl  aby8yi  PepperidgeFarmMembas   t3_aby8yi   \n",
       "4   erxg37c  c3kr9t              Vaughawa   t3_c3kr9t   \n",
       "..      ...     ...                   ...         ...   \n",
       "95  eidcu8a  azw37m                 MJGOO  t1_eiayjsx   \n",
       "96  f9ec3fk  e4kgrj             scps53770  t1_f9ebuvh   \n",
       "97  exw0qwv  cufufd               bothicc  t1_exv7rvj   \n",
       "98  ew69zua  cmykhy      eggplantsaredope   t3_cmykhy   \n",
       "99  ezsrtr6  d1zd4f          No_H_in_Cage  t1_ezsrtjq   \n",
       "\n",
       "                                                 body  sentiment  prediction  \n",
       "0                                          r/wooooosh         -1          -1  \n",
       "1      I'd keep away from him for a start. Good luck!         -1          -1  \n",
       "2   Oh. Sorry then. I just got a little confused f...         -1          -1  \n",
       "3   You never opened your mother’s present....what...         -1          -1  \n",
       "4                                             Amazing         -1          -1  \n",
       "..                                                ...        ...         ...  \n",
       "95                                88 miles an hour!!!         -1          -1  \n",
       "96         What the actual fuck did I just read Lmao!         -1          -1  \n",
       "97                                Me! Me! I want one!         -1          -1  \n",
       "98  Really weird that you couldnt bring the garden...         -1          -1  \n",
       "99  There is no 'H' in Nicolas Cage.\\n\\nSource: ht...         -1          -1  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This loads the entire user Comments into a Panda DataFrame\n",
    "#Note it has to be in the same directory as the notebook unless change\n",
    "#This also fills all the blank sentiment scores to -1\n",
    "comment_full = pd.read_csv('user_comments.csv')\n",
    "comment_full['sentiment'] = comment_full['sentiment'].fillna(-1)\n",
    "comment_full['prediction'] = -1\n",
    "\n",
    "#comment_df = comment_full[:2001]\n",
    "comment_df = comment_full\n",
    "random_df = comment_full[5001:5101]\n",
    "\n",
    "random_df = random_df.reset_index(drop=True)\n",
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section cleans up the code by removing false comments as well as comments that provide\n",
    "#little value (less < characters). The exception is key words like more/moar etc which indicates\n",
    "#the story was good.\n",
    "#THIS ONLY NEEDS TO BE RUN ONCE AS IT SAVES THE INFO BACK IN THE ORIGINAL FILE\n",
    "\n",
    "comment_df = comment_full\n",
    "comment_df['body'] = comment_df['body'].str.strip()\n",
    "comment_c = comment_df[~comment_df['body'].str.contains('remove')]\n",
    "comment_c = comment_c[~(comment_c['body'].str.lower()).str.contains('cake day')]\n",
    "comment_c = comment_c[~comment_c['body'].str.contains('It looks like there may be more to this story')]\n",
    "comment_c = comment_c[~comment_c['body'].str.contains('https://red')]\n",
    "comment_c = comment_c[comment_c['body'].str.len() >= 5]\n",
    "\n",
    "comment_more = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('more'))]\n",
    "comment_more[\"sentiment\"] = 1\n",
    "comment_moar = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('moar'))]\n",
    "comment_moar[\"sentiment\"] = 1\n",
    "comment_sick = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('sick'))]\n",
    "comment_sick[\"sentiment\"] = 1\n",
    "comment_holy = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('holy'))]\n",
    "comment_holy[\"sentiment\"] = 1\n",
    "\n",
    "comment_c = pd.concat([comment_c, comment_more])\n",
    "comment_c = pd.concat([comment_c, comment_moar])\n",
    "comment_c = pd.concat([comment_c, comment_sick])\n",
    "comment_c = pd.concat([comment_c, comment_holy])\n",
    "\n",
    "comment_df = comment_c\n",
    "\n",
    "comment_c.to_csv (r'user_comments.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes the rated comments and splits them into a positive\n",
    "#and negative data set. It then uses NLTK tweet tokenizer\n",
    "#to tokenize the comments of each user storing them into\n",
    "#a list of bag of words\n",
    "#NB - Positive Comments = 1. 'Not' Positive Comments = 0\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "positive_comments = comment_df[comment_df['sentiment'] == 1]\n",
    "negative_comments = comment_df[comment_df['sentiment'] == 0]\n",
    "\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for count in range(len(positive_comments)):\n",
    "    positive_tokens.append(tknzr.tokenize(positive_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(negative_comments)):\n",
    "    negative_tokens.append(tknzr.tokenize(negative_comments.iloc[count][\"body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section cleanses and normalizes the positive and negative tokens obtained above\n",
    "#Note that the cleansing part still needs some work.\n",
    "def remove_noise(comment_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(comment_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token != '...' and token != '…' and token != '’' and token != '‘' and token != '..' and token not in string.punctuation and token.lower() not in stop_words and token.lower() != \"#x200b\":            \n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads nltk's stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "negative_tokens_c = []\n",
    "positive_tokens_c = []\n",
    "\n",
    "for count in range(len(negative_tokens)):\n",
    "    negative_tokens_c.append(remove_noise(negative_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(positive_tokens)):\n",
    "    positive_tokens_c.append(remove_noise(positive_tokens[count], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section takes the cleansed tokens and put them into\n",
    "#dictionary objects to be used later on in the models\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "            \n",
    "def get_comments_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "pos_tokens_mod = get_comments_for_model(positive_tokens_c)\n",
    "neg_tokens_mod = get_comments_for_model(negative_tokens_c)\n",
    "\n",
    "#Below shows the top 10 common words in the Positive and\n",
    "#Negative Bag of words. This can be commented out once satisfied\n",
    "all_pos_words = get_all_words(positive_tokens_c)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "all_neg_words = get_all_words(negative_tokens_c)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB - You CAN'T run this multiple times in a row. It basically removes the data from the\n",
    "# *_tokens_mod variables. If you need to rerun this, please rerun the code above first\n",
    "# to repopulate these variables\n",
    "\n",
    "#This takes the positive/negative dictionaries created above and put them into data sets\n",
    "#It then merges them together, shuffle them and create train/test versions of the data sets\n",
    "#using 70/30% of the total data set respectively\n",
    "positive_dataset = [(comment_dict, \"Positive\")\n",
    "                     for comment_dict in pos_tokens_mod]\n",
    "\n",
    "negative_dataset = [(comment_dict, \"Negative\")\n",
    "                     for comment_dict in neg_tokens_mod]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "train_size = int(len(dataset)*0.8)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "#This is a sanity check to ensure that the total # equal to the comments that were rated\n",
    "print(len(positive_dataset), len(negative_dataset), len(dataset))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This stores the train and test data sets created into JSON files. These\n",
    "#files can be used later on to load into the model without having to go through\n",
    "#the work of cleaning up the data etc again\n",
    "with open('model_train.json', 'w') as json_file:\n",
    "    json.dump(train_data, json_file)\n",
    "\n",
    "with open('model_test.json', 'w') as json_file:\n",
    "    json.dump(test_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads the train and test data from the json files created into the\n",
    "#final objects to be loaded into the NaiveBayesClassifier model\n",
    "with open('model_train.json') as json_file:\n",
    "    train_data = json.load(json_file)\n",
    "\n",
    "with open('model_test.json') as json_file:\n",
    "    test_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.8046511627906977\n",
      "Most Informative Features\n",
      "                    holy = True           Positi : Negati =     16.4 : 1.0\n",
      "                 amazing = True           Positi : Negati =     13.8 : 1.0\n",
      "                   swear = True           Positi : Negati =      8.8 : 1.0\n",
      "                   await = True           Positi : Negati =      8.8 : 1.0\n",
      "                  writer = True           Positi : Negati =      8.8 : 1.0\n",
      "               fantastic = True           Positi : Negati =      8.1 : 1.0\n",
      "                   great = True           Positi : Negati =      8.1 : 1.0\n",
      "                   could = True           Negati : Positi =      7.6 : 1.0\n",
      "                 awesome = True           Positi : Negati =      7.0 : 1.0\n",
      "                    hook = True           Positi : Negati =      6.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#This creates a NaiveBayesClassifier model based on the train_data created\n",
    "#above then it tests it using the test_data set. Ideally, we want the accuracy\n",
    "#to be as high as possible. Right now it is about 55%\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section creates a new dataframe based on the un-rated comments\n",
    "#and then apply the model on the 1st 10 comments. The results of the analysis\n",
    "#is then shown with each comment. Once the model is more robust, this can\n",
    "#be updated to run against the entire comment list with the predicted value\n",
    "#stored in the prediction column\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for count in range(100):\n",
    "    custom_tokens = remove_noise(tknzr.tokenize(random_df.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))    \n",
    "    \n",
    "    predictions.append(1 if predicted_sentiment == 'Positive' else 0)\n",
    "\n",
    "random_df['prediction'] = predictions\n",
    "    \n",
    "random_df.to_csv (r'user_comments_model_val.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section creates a new dataframe based on the un-rated comments\n",
    "#and then apply the model on the 1st 10 comments. The results of the analysis\n",
    "#is then shown with each comment. Once the model is more robust, this can\n",
    "#be updated to run against the entire comment list with the predicted value\n",
    "#stored in the prediction column\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "predictions = []\n",
    "comment_df = comment_full\n",
    "\n",
    "for count in range(len(comment_df)):\n",
    "    if (comment_df.iloc[count][\"sentiment\"] == 1 or comment_df.iloc[count][\"sentiment\"] == 0):\n",
    "        predictions.append(comment_df.iloc[count][\"sentiment\"])\n",
    "        continue\n",
    "        \n",
    "    custom_tokens = remove_noise(tknzr.tokenize(comment_df.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))    \n",
    "    \n",
    "    predictions.append(1 if predicted_sentiment == 'Positive' else 0)\n",
    "\n",
    "comment_df['prediction'] = predictions\n",
    "    \n",
    "comment_df.to_csv (r'user_comments.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
