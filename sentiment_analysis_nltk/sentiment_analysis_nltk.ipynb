{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following tutorial at https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#shows how to do sentiment analysis using the nltk toolkit for python. This was used for the comment dataset for NoSleep\n",
    "#https://www.nltk.org/api/nltk.tokenize.html was also used\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only has to be run once. It downloads the modules that are necessary\n",
    "#to perform sentiment analysis\n",
    "\n",
    "#The punkt module is a pre-trained model that helps you tokenize words and sentences.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_5s</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_5s  prediction\n",
       "0          -1.0           2\n",
       "1           0.0          73\n",
       "2           1.0         251\n",
       "3           2.0        1381\n",
       "4           3.0         301\n",
       "5           4.0          98"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This loads the entire user Comments into a Panda DataFrame\n",
    "#Note it has to be in the same directory as the notebook unless change\n",
    "#This also fills all the blank sentiment scores to -1\n",
    "comment_full = pd.read_csv('user_comments.csv')\n",
    "comment_full['sentiment'] = comment_full['sentiment'].fillna(-1)\n",
    "#comment_full['prediction'] = -1\n",
    "comment_full['prediction_5s'] = -1\n",
    "\n",
    "random_story = comment_full[comment_full['storyId'].str.contains('bhokru')]\n",
    "random_story = random_story[random_story['prediction'] == 1]\n",
    "\n",
    "comment_full = comment_full[~comment_full['storyId'].str.contains('bhokru')]\n",
    "\n",
    "comment_df = comment_full[:2000]\n",
    "#comment_df = comment_full\n",
    "random_df = comment_full[2001:2101]\n",
    "\n",
    "random_df = random_df.reset_index(drop=True)\n",
    "#random_df\n",
    "\n",
    "comment_type = comment_full.groupby(\"sentiment_5s\")[\"prediction\"].count().reset_index().rename(columns={'prediction_y':'count'})\n",
    "comment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section cleans up the code by removing false comments as well as comments that provide\n",
    "#little value (less < characters). The exception is key words like more/moar etc which indicates\n",
    "#the story was good.\n",
    "#THIS ONLY NEEDS TO BE RUN ONCE AS IT SAVES THE INFO BACK IN THE ORIGINAL FILE\n",
    "\n",
    "comment_df = comment_full\n",
    "comment_df['body'] = comment_df['body'].str.strip()\n",
    "comment_c = comment_df[~comment_df['body'].str.contains('remove')]\n",
    "comment_c = comment_c[~(comment_c['body'].str.lower()).str.contains('cake day')]\n",
    "comment_c = comment_c[~comment_c['body'].str.contains('It looks like there may be more to this story')]\n",
    "comment_c = comment_c[~comment_c['body'].str.contains('https://red')]\n",
    "comment_c = comment_c[comment_c['body'].str.len() >= 5]\n",
    "\n",
    "comment_more = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('more'))]\n",
    "comment_more[\"sentiment\"] = 1\n",
    "comment_moar = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('moar'))]\n",
    "comment_moar[\"sentiment\"] = 1\n",
    "comment_sick = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('sick'))]\n",
    "comment_sick[\"sentiment\"] = 1\n",
    "comment_holy = comment_df[(comment_df['body'].str.len() < 5) & ((comment_df['body'].str.lower()).str.contains('holy'))]\n",
    "comment_holy[\"sentiment\"] = 1\n",
    "\n",
    "comment_c = pd.concat([comment_c, comment_more])\n",
    "comment_c = pd.concat([comment_c, comment_moar])\n",
    "comment_c = pd.concat([comment_c, comment_sick])\n",
    "comment_c = pd.concat([comment_c, comment_holy])\n",
    "\n",
    "comment_df = comment_c\n",
    "\n",
    "comment_c.to_csv (r'user_comments.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes the rated comments and splits them into a positive\n",
    "#and negative data set. It then uses NLTK tweet tokenizer\n",
    "#to tokenize the comments of each user storing them into\n",
    "#a list of bag of words\n",
    "#NB - Positive Comments = 1. 'Not' Positive Comments = 0\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "#positive_comments = comment_df[comment_df['sentiment'] == 1]\n",
    "#negative_comments = comment_df[comment_df['sentiment'] == 0]\n",
    "\n",
    "v_positive_comments = comment_df[comment_df['sentiment_5s'] == 4]\n",
    "positive_comments = comment_df[comment_df['sentiment_5s'] == 3]\n",
    "neutral_comments = comment_df[comment_df['sentiment_5s'] == 2]\n",
    "negative_comments = comment_df[comment_df['sentiment_5s'] == 1]\n",
    "v_negative_comments = comment_df[comment_df['sentiment_5s'] == 0]\n",
    "\n",
    "v_positive_comments = v_positive_comments#[:301]\n",
    "positive_comments = positive_comments#[:301]\n",
    "neutral_comments = neutral_comments#[:301]\n",
    "negative_comments = negative_comments#[:301]\n",
    "v_negative_comments = v_negative_comments#[:301]\n",
    "\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "v_positive_tokens = []\n",
    "v_negative_tokens = []\n",
    "neutral_tokens = []\n",
    "\n",
    "for count in range(len(positive_comments)):\n",
    "    #if (len(positive_comments.iloc[count][\"body\"]) <= 50):\n",
    "    positive_tokens.append(tknzr.tokenize(positive_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(negative_comments)):\n",
    "    negative_tokens.append(tknzr.tokenize(negative_comments.iloc[count][\"body\"]))\n",
    "        \n",
    "for count in range(len(v_positive_comments)):\n",
    "    v_positive_tokens.append(tknzr.tokenize(v_positive_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(v_negative_comments)):\n",
    "    v_negative_tokens.append(tknzr.tokenize(v_negative_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(neutral_comments)):\n",
    "    neutral_tokens.append(tknzr.tokenize(neutral_comments.iloc[count][\"body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section cleanses and normalizes the positive and negative tokens obtained above\n",
    "#Note that the cleansing part still needs some work.\n",
    "def remove_noise(comment_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(comment_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token != '...' and token != '…' and token != '’' and token != '‘' and token != '..' and token not in string.punctuation and token.lower() not in stop_words and token.lower() != \"#x200b\":            \n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 287 1311 238 68\n"
     ]
    }
   ],
   "source": [
    "#This loads nltk's stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "negative_tokens_c = []\n",
    "positive_tokens_c = []\n",
    "v_negative_tokens_c = []\n",
    "v_positive_tokens_c = []\n",
    "neutral_tokens_c = []\n",
    "\n",
    "for count in range(len(negative_tokens)):\n",
    "    negative_tokens_c.append(remove_noise(negative_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(positive_tokens)):\n",
    "    positive_tokens_c.append(remove_noise(positive_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(v_negative_tokens)):\n",
    "    v_negative_tokens_c.append(remove_noise(v_negative_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(v_positive_tokens)):\n",
    "    v_positive_tokens_c.append(remove_noise(v_positive_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(neutral_tokens)):\n",
    "    neutral_tokens_c.append(remove_noise(neutral_tokens[count], stop_words))\n",
    "    \n",
    "print(len(v_positive_tokens_c), len(positive_tokens_c), len(neutral_tokens_c), len(negative_tokens_c), len(v_negative_tokens_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('story', 69), ('read', 43), ('good', 37), ('part', 30), ('like', 27), ('love', 26), ('please', 23), ('get', 23), ('one', 22), ('wait', 21)]\n",
      "[('think', 32), ('like', 27), ('story', 23), ('get', 20), ('know', 18), (\"i'm\", 18), ('r', 18), ('people', 15), ('read', 15), ('thing', 14)]\n",
      "[('get', 183), ('like', 169), ('know', 159), ('go', 155), ('would', 127), ('think', 119), ('one', 113), ('say', 105), (\"i'm\", 97), ('good', 96)]\n"
     ]
    }
   ],
   "source": [
    "#This section takes the cleansed tokens and put them into\n",
    "#dictionary objects to be used later on in the models\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "            \n",
    "def get_comments_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "pos_tokens_mod = get_comments_for_model(positive_tokens_c)\n",
    "neg_tokens_mod = get_comments_for_model(negative_tokens_c)\n",
    "v_pos_tokens_mod = get_comments_for_model(v_positive_tokens_c)\n",
    "v_neg_tokens_mod = get_comments_for_model(v_negative_tokens_c)\n",
    "neu_tokens_mod = get_comments_for_model(neutral_tokens_c)\n",
    "\n",
    "#Below shows the top 10 common words in the Positive and\n",
    "#Negative Bag of words. This can be commented out once satisfied\n",
    "all_pos_words = get_all_words(positive_tokens_c)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "all_neg_words = get_all_words(negative_tokens_c)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "all_neu_words = get_all_words(neutral_tokens_c)\n",
    "freq_dist_neu = FreqDist(all_neu_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))\n",
    "print(freq_dist_neu.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 287 1311 238 68 1998\n",
      "1598 400\n"
     ]
    }
   ],
   "source": [
    "#NB - You CAN'T run this multiple times in a row. It basically removes the data from the\n",
    "# *_tokens_mod variables. If you need to rerun this, please rerun the code above first\n",
    "# to repopulate these variables\n",
    "\n",
    "#This takes the positive/negative dictionaries created above and put them into data sets\n",
    "#It then merges them together, shuffle them and create train/test versions of the data sets\n",
    "#using 70/30% of the total data set respectively\n",
    "positive_dataset = [(comment_dict, \"Positive\")\n",
    "                     for comment_dict in pos_tokens_mod]\n",
    "\n",
    "negative_dataset = [(comment_dict, \"Negative\")\n",
    "                     for comment_dict in neg_tokens_mod]\n",
    "\n",
    "v_positive_dataset = [(comment_dict, \"Very Positive\")\n",
    "                      for comment_dict in v_pos_tokens_mod]\n",
    "\n",
    "v_negative_dataset = [(comment_dict, \"Very Negative\")\n",
    "                      for comment_dict in v_neg_tokens_mod]\n",
    "\n",
    "neutral_dataset = [(comment_dict, \"Neutral\")\n",
    "                   for comment_dict in neu_tokens_mod]\n",
    "\n",
    "dataset = v_positive_dataset + positive_dataset + neutral_dataset + negative_dataset + v_negative_dataset\n",
    "train_size = int(len(dataset)*0.8)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "#This is a sanity check to ensure that the total # equal to the comments that were rated\n",
    "#print(len(positive_dataset), len(negative_dataset), len(dataset))\n",
    "print(len(v_positive_dataset), len(positive_dataset), len(neutral_dataset), len(negative_dataset), len(v_negative_dataset), len(dataset))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This stores the train and test data sets created into JSON files. These\n",
    "#files can be used later on to load into the model without having to go through\n",
    "#the work of cleaning up the data etc again\n",
    "with open('model_train_nltk.json', 'w') as json_file:\n",
    "    json.dump(train_data, json_file)\n",
    "\n",
    "with open('model_test_nltk.json', 'w') as json_file:\n",
    "    json.dump(test_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads the train and test data from the json files created into the\n",
    "#final objects to be loaded into the NaiveBayesClassifier model\n",
    "with open('model_train_nltk.json') as json_file:\n",
    "    train_data = json.load(json_file)\n",
    "\n",
    "with open('model_test_nltk.json') as json_file:\n",
    "    test_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.395\n",
      "Most Informative Features\n",
      "                    holy = True           Very P : Neutra =     77.1 : 1.0\n",
      "               fantastic = True           Very P : Neutra =     52.4 : 1.0\n",
      "                   twist = True           Very N : Neutra =     33.9 : 1.0\n",
      "                     wtf = True           Very N : Neutra =     33.9 : 1.0\n",
      "                    lmao = True           Very N : Neutra =     33.9 : 1.0\n",
      "                terrible = True           Very N : Neutra =     33.9 : 1.0\n",
      "                    fake = True           Very N : Neutra =     33.9 : 1.0\n",
      "                 awesome = True           Very P : Neutra =     26.5 : 1.0\n",
      "              incredible = True           Very P : Neutra =     23.8 : 1.0\n",
      "                 terrify = True           Very P : Neutra =     23.8 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#This creates a NaiveBayesClassifier model based on the train_data created\n",
    "#above then it tests it using the test_data set. Ideally, we want the accuracy\n",
    "#to be as high as possible. Right now it is about 55%\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section creates a new dataframe based on the un-rated comments\n",
    "#and then apply the model on the 1st 10 comments. The results of the analysis\n",
    "#is then shown with each comment. Once the model is more robust, this can\n",
    "#be updated to run against the entire comment list with the predicted value\n",
    "#stored in the prediction column\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for count in range(100):\n",
    "#     if (len(random_df.iloc[count][\"body\"]) >= 50):\n",
    "#         predictions.append(0)\n",
    "#         continue\n",
    "        \n",
    "    custom_tokens = remove_noise(tknzr.tokenize(random_df.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "    \n",
    "    if predicted_sentiment == \"Very Positive\":\n",
    "        predictions.append(4)\n",
    "\n",
    "    if predicted_sentiment == \"Positive\":\n",
    "        predictions.append(3)\n",
    "\n",
    "    if predicted_sentiment == \"Neutral\":\n",
    "        predictions.append(2)\n",
    "\n",
    "    if predicted_sentiment == \"Negative\":\n",
    "        predictions.append(1)\n",
    "\n",
    "    if predicted_sentiment == \"Very Negative\":\n",
    "        predictions.append(0)\n",
    "    \n",
    "    #predictions.append(1 if predicted_sentiment == 'Positive' else 0)\n",
    "\n",
    "random_df['prediction_5s'] = predictions\n",
    "    \n",
    "random_df.to_csv (r'user_comments_model_val.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section creates a new dataframe based on the un-rated comments\n",
    "#and then apply the model on the 1st 10 comments. The results of the analysis\n",
    "#is then shown with each comment. Once the model is more robust, this can\n",
    "#be updated to run against the entire comment list with the predicted value\n",
    "#stored in the prediction column\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "predictions = []\n",
    "comment_df = comment_full\n",
    "\n",
    "for count in range(len(comment_df)):\n",
    "    if (comment_df.iloc[count][\"sentiment\"] == 1 or comment_df.iloc[count][\"sentiment\"] == 0):\n",
    "        predictions.append(comment_df.iloc[count][\"sentiment\"])\n",
    "        continue\n",
    "        \n",
    "    custom_tokens = remove_noise(tknzr.tokenize(comment_df.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))    \n",
    "    \n",
    "    predictions.append(1 if predicted_sentiment == 'Positive' else 0)\n",
    "\n",
    "comment_df['prediction'] = predictions\n",
    "    \n",
    "comment_df.to_csv (r'user_comments.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>u_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#NAME?</td>\n",
       "      <td>4.795791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---REDACTED----</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--Paradigm--</td>\n",
       "      <td>3.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--Yama--</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4-Z-N-</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14218</th>\n",
       "      <td>zorothex</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14219</th>\n",
       "      <td>zrednaxelaz1222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14220</th>\n",
       "      <td>zschutte10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14221</th>\n",
       "      <td>zxh01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14222</th>\n",
       "      <td>zzsparkzz</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14223 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  u_comments\n",
       "0               #NAME?    4.795791\n",
       "1      ---REDACTED----    0.000000\n",
       "2         --Paradigm--    3.218876\n",
       "3             --Yama--    0.000000\n",
       "4              -4-Z-N-    1.386294\n",
       "...                ...         ...\n",
       "14218         zorothex    0.000000\n",
       "14219  zrednaxelaz1222    0.000000\n",
       "14220       zschutte10    0.000000\n",
       "14221            zxh01    0.000000\n",
       "14222        zzsparkzz    2.197225\n",
       "\n",
       "[14223 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_comments = pd.merge(random_story, comment_full, on='author', how='inner')\n",
    "pos_comments = pos_comments[pos_comments['prediction_y'] == 1]\n",
    "pos_comments = pos_comments.filter(like='_y')\n",
    "\n",
    "story_power = pos_comments.groupby(\"storyId_y\")[\"prediction_y\"].count().reset_index().rename(columns={'prediction_y':'power'})\n",
    "story_power['power'] = 4**(story_power['power'])\n",
    "\n",
    "story_comments = comment_full[comment_full['prediction'] == 1]\n",
    "story_comments = story_comments.groupby(\"storyId\")[\"prediction\"].count().reset_index().rename(columns={'prediction':'comments'})\n",
    "\n",
    "user_comments = comment_full[comment_full['prediction'] == 1]\n",
    "user_comments = user_comments.groupby(\"author\")[\"prediction\"].count().reset_index().rename(columns={'prediction':'u_comments'})\n",
    "user_comments['u_comments'] = np.log(user_comments['u_comments']) * 2\n",
    "\n",
    "user_comments\n",
    "\n",
    "#story_comments\n",
    "#user_comments.to_csv (r'user_comments_c.csv', index = False, header=True)\n",
    "\n",
    "#Formula - 5^(Commenters liking same story - 1) + log(Commenters Comments) + Story Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
