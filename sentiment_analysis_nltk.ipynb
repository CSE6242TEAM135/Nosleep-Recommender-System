{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following tutorial at https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "#shows how to do sentiment analysis using the nltk toolkit for python. This was used for the comment dataset for NoSleep\n",
    "#https://www.nltk.org/api/nltk.tokenize.html was also used\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This only has to be run once. It downloads the modules that are necessary\n",
    "#to perform sentiment analysis\n",
    "\n",
    "#The punkt module is a pre-trained model that helps you tokenize words and sentences.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>sortKey</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>id</th>\n",
       "      <th>storyId</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bs22s7</td>\n",
       "      <td>1558760400</td>\n",
       "      <td>10</td>\n",
       "      <td>/r/nosleep/comments/bs22s7/i_work_on_a_boat_ou...</td>\n",
       "      <td>t2_wcuxx</td>\n",
       "      <td>eok7qb9</td>\n",
       "      <td>bs22s7</td>\n",
       "      <td>Wolf_of_WV</td>\n",
       "      <td>t3_bs22s7</td>\n",
       "      <td>You are a dead man walking.  The people who ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_dukiqw</td>\n",
       "      <td>1573621200</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/dukiqw/a_childs_method_for...</td>\n",
       "      <td>t2_86jlh</td>\n",
       "      <td>f77papf</td>\n",
       "      <td>dukiqw</td>\n",
       "      <td>Sporkazm</td>\n",
       "      <td>t1_f77p7a1</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\nSomehow I came to return the e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_e4lcwk</td>\n",
       "      <td>1575349200</td>\n",
       "      <td>4</td>\n",
       "      <td>/r/nosleep/comments/e4lcwk/everyone_knows_the_...</td>\n",
       "      <td>t2_j5mx0</td>\n",
       "      <td>f9erm02</td>\n",
       "      <td>e4lcwk</td>\n",
       "      <td>LiKenun</td>\n",
       "      <td>t3_e4lcwk</td>\n",
       "      <td>&amp;gt;My neck was sticky, and stank, stank  like...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_au1bdu</td>\n",
       "      <td>1551157200</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/au1bdu/conditions_of_entry...</td>\n",
       "      <td>t2_nguj2</td>\n",
       "      <td>eh6cdqn</td>\n",
       "      <td>au1bdu</td>\n",
       "      <td>Reddit__Herring</td>\n",
       "      <td>t3_au1bdu</td>\n",
       "      <td>That was really awesome. Very interesting conc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_d64uh2</td>\n",
       "      <td>1568955600</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/d64uh2/the_188minute_man/f...</td>\n",
       "      <td>t2_2vpmh2gy</td>\n",
       "      <td>f0rojyz</td>\n",
       "      <td>d64uh2</td>\n",
       "      <td>jcammarato</td>\n",
       "      <td>t1_f0ridzm</td>\n",
       "      <td>Yes, but she also has no choice and will event...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169583</th>\n",
       "      <td>t3_b8zif8</td>\n",
       "      <td>1554440400</td>\n",
       "      <td>7</td>\n",
       "      <td>/r/nosleep/comments/b8zif8/the_lynch_house/ek2...</td>\n",
       "      <td>t2_20igrc4i</td>\n",
       "      <td>ek27hib</td>\n",
       "      <td>b8zif8</td>\n",
       "      <td>BlondeRR1717</td>\n",
       "      <td>t3_b8zif8</td>\n",
       "      <td>Are you aware that you wrote natural causes fo...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169584</th>\n",
       "      <td>t3_bgj62b</td>\n",
       "      <td>1556168400</td>\n",
       "      <td>73</td>\n",
       "      <td>/r/nosleep/comments/bgj62b/my_first_breath_too...</td>\n",
       "      <td>t2_lg9ark7</td>\n",
       "      <td>ellzl0a</td>\n",
       "      <td>bgj62b</td>\n",
       "      <td>Bismuthie</td>\n",
       "      <td>t1_ellx4dp</td>\n",
       "      <td>Omg smart</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169585</th>\n",
       "      <td>t3_btonzl</td>\n",
       "      <td>1559106000</td>\n",
       "      <td>2</td>\n",
       "      <td>/r/nosleep/comments/btonzl/her_eye_was_a_spira...</td>\n",
       "      <td>t2_gkau4</td>\n",
       "      <td>ep15bo3</td>\n",
       "      <td>btonzl</td>\n",
       "      <td>thejollyden</td>\n",
       "      <td>t1_ep12y0d</td>\n",
       "      <td>How do you delete comments?</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169586</th>\n",
       "      <td>t3_cn3nbl</td>\n",
       "      <td>1565326800</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/cn3nbl/straight_to_vhs_sun...</td>\n",
       "      <td>t2_17gt7w</td>\n",
       "      <td>ew7ctpd</td>\n",
       "      <td>cn3nbl</td>\n",
       "      <td>LadyGrey1174</td>\n",
       "      <td>t3_cn3nbl</td>\n",
       "      <td>Holy hannah, time for a Disney movie...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169587</th>\n",
       "      <td>t3_dh11jg</td>\n",
       "      <td>1571202000</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/dh11jg/my_friend_and_i_fou...</td>\n",
       "      <td>t2_21i87yt2</td>\n",
       "      <td>f3oyfms</td>\n",
       "      <td>dh11jg</td>\n",
       "      <td>TheCorrectAyhZad</td>\n",
       "      <td>t1_f3ju9rf</td>\n",
       "      <td>A cult dedicated to breeding Homo Sapiens and ...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169588 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          link_id     sortKey  score  \\\n",
       "0       t3_bs22s7  1558760400     10   \n",
       "1       t3_dukiqw  1573621200      1   \n",
       "2       t3_e4lcwk  1575349200      4   \n",
       "3       t3_au1bdu  1551157200      1   \n",
       "4       t3_d64uh2  1568955600      1   \n",
       "...           ...         ...    ...   \n",
       "169583  t3_b8zif8  1554440400      7   \n",
       "169584  t3_bgj62b  1556168400     73   \n",
       "169585  t3_btonzl  1559106000      2   \n",
       "169586  t3_cn3nbl  1565326800      1   \n",
       "169587  t3_dh11jg  1571202000      1   \n",
       "\n",
       "                                                permalink author_fullname  \\\n",
       "0       /r/nosleep/comments/bs22s7/i_work_on_a_boat_ou...        t2_wcuxx   \n",
       "1       /r/nosleep/comments/dukiqw/a_childs_method_for...        t2_86jlh   \n",
       "2       /r/nosleep/comments/e4lcwk/everyone_knows_the_...        t2_j5mx0   \n",
       "3       /r/nosleep/comments/au1bdu/conditions_of_entry...        t2_nguj2   \n",
       "4       /r/nosleep/comments/d64uh2/the_188minute_man/f...     t2_2vpmh2gy   \n",
       "...                                                   ...             ...   \n",
       "169583  /r/nosleep/comments/b8zif8/the_lynch_house/ek2...     t2_20igrc4i   \n",
       "169584  /r/nosleep/comments/bgj62b/my_first_breath_too...      t2_lg9ark7   \n",
       "169585  /r/nosleep/comments/btonzl/her_eye_was_a_spira...        t2_gkau4   \n",
       "169586  /r/nosleep/comments/cn3nbl/straight_to_vhs_sun...       t2_17gt7w   \n",
       "169587  /r/nosleep/comments/dh11jg/my_friend_and_i_fou...     t2_21i87yt2   \n",
       "\n",
       "             id storyId            author   parent_id  \\\n",
       "0       eok7qb9  bs22s7        Wolf_of_WV   t3_bs22s7   \n",
       "1       f77papf  dukiqw          Sporkazm  t1_f77p7a1   \n",
       "2       f9erm02  e4lcwk           LiKenun   t3_e4lcwk   \n",
       "3       eh6cdqn  au1bdu   Reddit__Herring   t3_au1bdu   \n",
       "4       f0rojyz  d64uh2        jcammarato  t1_f0ridzm   \n",
       "...         ...     ...               ...         ...   \n",
       "169583  ek27hib  b8zif8      BlondeRR1717   t3_b8zif8   \n",
       "169584  ellzl0a  bgj62b         Bismuthie  t1_ellx4dp   \n",
       "169585  ep15bo3  btonzl       thejollyden  t1_ep12y0d   \n",
       "169586  ew7ctpd  cn3nbl      LadyGrey1174   t3_cn3nbl   \n",
       "169587  f3oyfms  dh11jg  TheCorrectAyhZad  t1_f3ju9rf   \n",
       "\n",
       "                                                     body  sentiment  \\\n",
       "0       You are a dead man walking.  The people who ar...        0.0   \n",
       "1       &amp;#x200B;\\n\\nSomehow I came to return the e...        0.0   \n",
       "2       &gt;My neck was sticky, and stank, stank  like...        1.0   \n",
       "3       That was really awesome. Very interesting conc...        1.0   \n",
       "4       Yes, but she also has no choice and will event...       -1.0   \n",
       "...                                                   ...        ...   \n",
       "169583  Are you aware that you wrote natural causes fo...       -1.0   \n",
       "169584                                          Omg smart       -1.0   \n",
       "169585                        How do you delete comments?       -1.0   \n",
       "169586            Holy hannah, time for a Disney movie...       -1.0   \n",
       "169587  A cult dedicated to breeding Homo Sapiens and ...       -1.0   \n",
       "\n",
       "        prediction  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3               -1  \n",
       "4               -1  \n",
       "...            ...  \n",
       "169583          -1  \n",
       "169584          -1  \n",
       "169585          -1  \n",
       "169586          -1  \n",
       "169587          -1  \n",
       "\n",
       "[169588 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This loads the entire user Comments into a Panda DataFrame\n",
    "#Note it has to be in the same directory as the notebook unless change\n",
    "#This also fills all the blank sentiment scores to -1\n",
    "comment_df = pd.read_csv('user_comments.csv')\n",
    "comment_df['sentiment'] = comment_df['sentiment'].fillna(-1)\n",
    "\n",
    "comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This takes the rated comments and splits them into a positive\n",
    "#and negative data set. It then uses NLTK tweet tokenizer\n",
    "#to tokenize the comments of each user storing them into\n",
    "#a list of bag of words\n",
    "#NB - Positive Comments = 1. 'Not' Positive Comments = 0\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "positive_comments = comment_df[comment_df['sentiment'] == 1]\n",
    "negative_comments = comment_df[comment_df['sentiment'] == 0]\n",
    "\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for count in range(len(positive_comments)):\n",
    "    positive_tokens.append(tknzr.tokenize(positive_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(negative_comments)):\n",
    "    negative_tokens.append(tknzr.tokenize(negative_comments.iloc[count][\"body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section cleanses and normalizes the positive and negative tokens obtained above\n",
    "#Note that the cleansing part still needs some work.\n",
    "def remove_noise(comment_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(comment_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "#This loads nltk's stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "negative_tokens_c = []\n",
    "positive_tokens_c = []\n",
    "\n",
    "for count in range(len(negative_tokens)):\n",
    "    negative_tokens_c.append(remove_noise(negative_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(positive_tokens)):\n",
    "    positive_tokens_c.append(remove_noise(positive_tokens[count], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('’', 27), ('get', 17), ('good', 14), ('like', 12), ('read', 12), ('kill', 11), ('think', 11), ('go', 10), ('op', 10), (\"i'm\", 10)]\n",
      "[('like', 15), ('r', 15), ('nosleep', 15), ('#x200b', 14), ('get', 14), ('question', 14), ('must', 14), ('message', 14), ('moderator', 14), ('submission', 13)]\n"
     ]
    }
   ],
   "source": [
    "#This section takes the cleansed tokens and put them into\n",
    "#dictionary objects to be used later on in the models\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "            \n",
    "def get_comments_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "pos_tokens_mod = get_comments_for_model(positive_tokens_c)\n",
    "neg_tokens_mod = get_comments_for_model(negative_tokens_c)\n",
    "\n",
    "#Below shows the top 10 common words in the Positive and\n",
    "#Negative Bag of words. This can be commented out once satisfied\n",
    "all_pos_words = get_all_words(positive_tokens_c)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "all_neg_words = get_all_words(negative_tokens_c)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 89 196\n",
      "137 59\n"
     ]
    }
   ],
   "source": [
    "#NB - You CAN'T run this multiple times in a row. It basically removes the data from the\n",
    "# *_tokens_mod variables. If you need to rerun this, please rerun the code above first\n",
    "# to repopulate these variables\n",
    "\n",
    "#This takes the positive/negative dictionaries created above and put them into data sets\n",
    "#It then merges them together, shuffle them and create train/test versions of the data sets\n",
    "#using 70/30% of the total data set respectively\n",
    "positive_dataset = [(comment_dict, \"Positive\")\n",
    "                     for comment_dict in pos_tokens_mod]\n",
    "\n",
    "negative_dataset = [(comment_dict, \"Negative\")\n",
    "                     for comment_dict in neg_tokens_mod]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "train_size = int(len(dataset)*0.7)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "#This is a sanity check to ensure that the total # equal to the comments that were rated\n",
    "print(len(positive_dataset), len(negative_dataset), len(dataset))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.5423728813559322\n",
      "Most Informative Features\n",
      "                    post = True           Negati : Positi =      7.0 : 1.0\n",
      "               subreddit = True           Negati : Positi =      7.0 : 1.0\n",
      "                       r = True           Negati : Positi =      7.0 : 1.0\n",
      "                    must = True           Negati : Positi =      7.0 : 1.0\n",
      "                    look = True           Negati : Positi =      6.1 : 1.0\n",
      "                 comment = True           Negati : Positi =      5.1 : 1.0\n",
      "                   check = True           Negati : Positi =      4.2 : 1.0\n",
      "                    need = True           Negati : Positi =      3.6 : 1.0\n",
      "                    kind = True           Negati : Positi =      3.3 : 1.0\n",
      "                    part = True           Negati : Positi =      3.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#This creates a NaiveBayesClassifier model based on the train_data created\n",
    "#above then it tests it using the test_data set. Ideally, we want the accuracy\n",
    "#to be as high as possible. Right now it is about 55%\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Negative - Yes, but she also has no choice and will eventually crave a companion as well.\n",
      "2. Negative - I'm glad your husband decided to confront her. You would have never known otherwise.\n",
      "3. Negative - Thank you. I don’t know what I believe anymore, but I appreciate your words.\n",
      "4. Positive - Beautiful!\n",
      "5. Negative - I did in a comment,but they've allbeen removed by nosleep.\n",
      "\n",
      "I copy/pasted here for you:\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "The first was [removed for believability](https://www.reddit.com/r/nosleep/comments/c2kbxo/warningyouarenotsafe/). I didn't see anything that I thought it should be removed for. The second one is [here.](https://www.reddit.com/r/nosleep/comments/c2l80b/did_you_get_the_alert/)\n",
      "\n",
      "[This](https://www.reddit.com/r/nosleep/comments/c1wxxu/i_work_at_nasa_we_made_alien_contact_yesterday/) was posted two days ago...not saying it's related, not saying it isn't.\n",
      "\n",
      "But I'm scared.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "EDIT. Oh, holy shit. All alert posts have been removed. WTF.\n",
      "6. Negative - Happens with far more stories on this sub than it should. I don’t know if people get bored telling their true stories and rush the ending, or if the endings are just really hard to convey. Either way it’s disappointing every time it happens\n",
      "7. Negative - r/hydrohomies must be in on it\n",
      "8. Negative - I did not realize I was reading /r/nosleep and I thought this was an IAMA for a second\n",
      "9. Positive - So wholesome. Love it.\n",
      "10. Negative - No comment.\n"
     ]
    }
   ],
   "source": [
    "#This section creates a new dataframe based on the un-rated comments\n",
    "#and then apply the model on the 1st 10 comments. The results of the analysis\n",
    "#is then shown with each comment. Once the model is more robust, this can\n",
    "#be updated to run against the entire comment list with the predicted value\n",
    "#stored in the prediction column\n",
    "random_comments = comment_df[comment_df['sentiment'] == -1]\n",
    "random_comments\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "for count in range(10):\n",
    "    custom_tokens = remove_noise(tknzr.tokenize(random_comments.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "    \n",
    "    print(str(count + 1) + '. ' + predicted_sentiment + ' - ' + random_comments.iloc[count][\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
