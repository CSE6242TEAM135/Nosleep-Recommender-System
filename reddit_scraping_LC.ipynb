{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this, you need to install the following libraries:\n",
    "\n",
    "1) **Vader** - https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "   \n",
    "   *pip install vaderSentiment*\n",
    "   \n",
    "   This is used for the Sentiment Analysis\n",
    "   \n",
    "2) **Boto3** - https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.Python.03.html#GettingStarted.Python.03.01\n",
    "\n",
    "   *pip install boto3*\n",
    "   \n",
    "   This is used to read/write data into DynamoDB objects. Note you have to setup AWSCLI below first as it uses that for\n",
    "   the connection/user details.\n",
    "   \n",
    "3) **AWSCLI** - https://sysadmins.co.za/interfacing-amazon-dynamodb-with-python-using-boto3/\n",
    "\n",
    "   *pip install awscli*\n",
    "   \n",
    "   This is used to setup the connection/configuration parameters needed to access the DynamoDB objects. Sanjeev had provided \n",
    "   the details in his email and the instructions show you what to do under Lets get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import praw\n",
    "import boto3\n",
    "import json\n",
    "import decimal\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "from decimal import Decimal\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the PRAW api, follow the directions here https://www.storybench.org/how-to-scrape-reddit-with-python/\n",
    "Note this involves creating a Reddit Account and a Reddit App ID which the instructions guide you through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAL_USE_SCRIPT_14_CHARS = ''\n",
    "SECRET_KEY_27_CHARS = ''\n",
    "YOUR_APP_NAME = ''\n",
    "YOUR_REDDIT_USER_NAME = ''\n",
    "YOUR_REDDIT_LOGIN_PASSWORD = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper class to convert a DynamoDB item to JSON.\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            if abs(o) % 1 > 0:\n",
    "                return float(o)\n",
    "            else:\n",
    "                return int(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "\n",
    "# This class is used to handle all of the interactions (uploading/downloading tables etc) from\n",
    "# the DynamoDB tables setup for the project\n",
    "class DynamoDBEngine:\n",
    "    def __init__(self):\n",
    "        #This creates the dynamoDB object that points to the location of the database\n",
    "        #Note this requires the AWSCLI connection details to be setup\n",
    "        self.dynamodb = boto3.resource('dynamodb', region_name='us-east-1', endpoint_url=\"https://dynamodb.us-east-1.amazonaws.com\")\n",
    "    \n",
    "    #This takes the comments panda dataframe and uploads comments to the UserComments\n",
    "    #table in DynamoDB one row at a time\n",
    "    def uploadComments(self, comment_df):        \n",
    "        table = self.dynamodb.Table('UserComments')\n",
    "        \n",
    "        for row in comment_df.itertuples():\n",
    "            response = table.put_item(\n",
    "                Item={\n",
    "                    'comment_id': row.comment_id,\n",
    "                    'story_id': row.story_id,\n",
    "                    'comment_author': row.comment_author,\n",
    "                    'comment_body': row.comment_body,\n",
    "                    'negative_sa_score': Decimal(str(row.negative_sa_score)),\n",
    "                    'neutral_sa_score': Decimal(str(row.neutral_sa_score)),\n",
    "                    'positive_sa_score': Decimal(str(row.positive_sa_score)),\n",
    "                    'compound_sa_score': Decimal(str(row.compound_sa_score))\n",
    "                })\n",
    "            \n",
    "            if response['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def downloadComments(self):        \n",
    "        comments_dict = {\"link_id\": [],\n",
    "                         \"sortKey\": [],\n",
    "                         \"score\": [],\n",
    "                         \"permalink\": [],\n",
    "                         \"author_fullname\": [],\n",
    "                         \"id\": [],\n",
    "                         \"storyId\": [],\n",
    "                         \"author\": [],\n",
    "                         \"parent_id\": [],\n",
    "                         \"body\": []\n",
    "                        }\n",
    "            \n",
    "        table = self.dynamodb.Table('CommentsNoSleep')\n",
    "        \n",
    "        itemsList = []\n",
    "        response = table.scan()\n",
    "        \n",
    "        for i in response['Items']:\n",
    "            for key in i.keys():                \n",
    "                comments_dict[key].append(i[key])\n",
    "            \n",
    "        while 'LastEvaluatedKey' in response:\n",
    "            response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "            \n",
    "            for i in response['Items']:\n",
    "                for key in i.keys():                \n",
    "                    comments_dict[key].append(i[key])\n",
    "                \n",
    "        df = pd.DataFrame(comments_dict)\n",
    "        return df        \n",
    "    \n",
    "    #This takes the comments panda dataframe and uploads comments to the UserComments\n",
    "    #table in DynamoDB one row at a time\n",
    "    def uploadStories(self, story_df):\n",
    "        table = self.dynamodb.Table('UserStories')\n",
    "        \n",
    "        for row in story_df.itertuples():\n",
    "            response = table.put_item(\n",
    "                Item={\n",
    "                    'story_id': row.story_id,\n",
    "                    'title': row.title,\n",
    "                    'author': row.author,\n",
    "                    'body': row.body\n",
    "                })\n",
    "            \n",
    "            if response['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "                return False\n",
    "        \n",
    "        return True       \n",
    "\n",
    "# This class is used to handle all of the interactions (downloading stories/comments) from\n",
    "# the NoSleep Subreddit\n",
    "class NoSleepRecommender:\n",
    "    def __init__(self):\n",
    "        self.reddit = praw.Reddit(client_id=PERSONAL_USE_SCRIPT_14_CHARS,\n",
    "                                  client_secret=SECRET_KEY_27_CHARS,\n",
    "                                  password=YOUR_REDDIT_LOGIN_PASSWORD,\n",
    "                                  user_agent=YOUR_APP_NAME,\n",
    "                                  username=YOUR_REDDIT_USER_NAME)\n",
    "\n",
    "        #print(self.reddit.user.me())\n",
    "\n",
    "        self.subreddit = self.reddit.subreddit('nosleep')\n",
    "\n",
    "        self.stories_dict = {\"story_id\": [],\n",
    "                        \"title\": [],\n",
    "                        \"author\": [],\n",
    "                        \"body\": []}\n",
    "        self.comments_dict = {\"comment_id\": [],\n",
    "                         \"story_id\": [],\n",
    "                         \"comment_author\": [],\n",
    "                         \"comment_body\": [],\n",
    "                         \"negative_sa_score\": [],\n",
    "                         \"neutral_sa_score\": [],\n",
    "                         \"positive_sa_score\": [],\n",
    "                         \"compound_sa_score\": []}\n",
    "    \n",
    "    def loadComments(self):\n",
    "        self.comment_df = pd.read_csv('user_comments.csv')\n",
    "        self.comment_df['sentiment'] = self.comment_df['sentiment'].fillna(-1)\n",
    "        \n",
    "    def loadStories(self):\n",
    "        self.story_df = pd.read_csv('user_stories.csv')\n",
    "        self.story_df = self.story_df.fillna(' ')\n",
    "        \n",
    "    def loadAll(self):\n",
    "        self.loadComments()\n",
    "        self.loadStories()\n",
    "        \n",
    "    def returnComments(self):\n",
    "        return self.comment_df\n",
    "    \n",
    "    def returnStories(self):\n",
    "        return self.story_df    \n",
    "    \n",
    "    def saveComments(self):\n",
    "        self.comment_df.to_csv (r'user_comments.csv', index = False, header=True)\n",
    "        \n",
    "    def saveStories(self):\n",
    "        self.story_df.to_csv (r'user_stories.csv', index = False, header=True)\n",
    "        \n",
    "    def saveAll(self):\n",
    "        self.saveComments()\n",
    "        self.saveStories()\n",
    "        \n",
    "    def getStories(self):\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "        my_subreddit = self.subreddit.hot(limit=10)\n",
    "        for submission in my_subreddit:\n",
    "            self.stories_dict[\"title\"].append(submission.title)\n",
    "            self.stories_dict[\"body\"].append(submission.selftext)\n",
    "            self.stories_dict[\"author\"].append(submission.author)\n",
    "            self.stories_dict[\"story_id\"].append(submission.id)\n",
    "            \n",
    "            submission.comments.replace_more(limit=None)\n",
    "            all_comments = submission.comments.list()            \n",
    "            \n",
    "            for comment in all_comments:\n",
    "                #This does the sentiment analysis and returns the\n",
    "                #scores obtained for the comment. A compound score is a\n",
    "                #one-dimensional assessment. If it is >= 0.05, then the comment\n",
    "                #is perceived as 'positive'. The individual scores show what %\n",
    "                #of the comment is neutral, +ve, and/or -ve\n",
    "                score = analyser.polarity_scores(comment.body)\n",
    "                \n",
    "                self.comments_dict[\"comment_id\"].append(comment.id)\n",
    "                self.comments_dict[\"story_id\"].append(submission.id)\n",
    "                self.comments_dict[\"comment_body\"].append(comment.body)\n",
    "                self.comments_dict[\"comment_author\"].append(comment.author)\n",
    "                self.comments_dict[\"negative_sa_score\"].append(score[\"neg\"])\n",
    "                self.comments_dict[\"neutral_sa_score\"].append(score[\"neu\"])\n",
    "                self.comments_dict[\"positive_sa_score\"].append(score[\"pos\"])\n",
    "                self.comments_dict[\"compound_sa_score\"].append(score[\"compound\"])\n",
    "\n",
    "        self.story_df = pd.DataFrame(self.stories_dict)\n",
    "        self.story_df = self.story_df.dropna()        \n",
    "        \n",
    "        self.comment_df = pd.DataFrame(self.comments_dict)\n",
    "        self.comment_df = comment_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this section to read stories/comments from the NoSleep Reddit and save them to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsapp = NoSleepRecommender()\n",
    "nsapp.getStories()\n",
    "nsapp.saveAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this section to load the stories/comments from the saved CSV files and save them to the DynamoDB tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsapp = NoSleepRecommender()\n",
    "nsapp.loadAll()\n",
    "\n",
    "comment_df = nsapp.returnComments()\n",
    "story_df = nsapp.returnStories()\n",
    "\n",
    "dbeng = DynamoDBEngine()\n",
    "\n",
    "dbeng.uploadStories(story_df)\n",
    "dbeng.uploadComments(comment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbeng = DynamoDBEngine()\n",
    "#comment_df = dbeng.downloadComments()\n",
    "\n",
    "nsapp = NoSleepRecommender()\n",
    "nsapp.loadComments()\n",
    "\n",
    "comment_df = nsapp.returnComments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>sortKey</th>\n",
       "      <th>score</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>id</th>\n",
       "      <th>storyId</th>\n",
       "      <th>author</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_bs22s7</td>\n",
       "      <td>1558760400</td>\n",
       "      <td>10</td>\n",
       "      <td>/r/nosleep/comments/bs22s7/i_work_on_a_boat_ou...</td>\n",
       "      <td>t2_wcuxx</td>\n",
       "      <td>eok7qb9</td>\n",
       "      <td>bs22s7</td>\n",
       "      <td>Wolf_of_WV</td>\n",
       "      <td>t3_bs22s7</td>\n",
       "      <td>You are a dead man walking.  The people who ar...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_dukiqw</td>\n",
       "      <td>1573621200</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/dukiqw/a_childs_method_for...</td>\n",
       "      <td>t2_86jlh</td>\n",
       "      <td>f77papf</td>\n",
       "      <td>dukiqw</td>\n",
       "      <td>Sporkazm</td>\n",
       "      <td>t1_f77p7a1</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\nSomehow I came to return the e...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_e4lcwk</td>\n",
       "      <td>1575349200</td>\n",
       "      <td>4</td>\n",
       "      <td>/r/nosleep/comments/e4lcwk/everyone_knows_the_...</td>\n",
       "      <td>t2_j5mx0</td>\n",
       "      <td>f9erm02</td>\n",
       "      <td>e4lcwk</td>\n",
       "      <td>LiKenun</td>\n",
       "      <td>t3_e4lcwk</td>\n",
       "      <td>&amp;gt;My neck was sticky, and stank, stank  like...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_au1bdu</td>\n",
       "      <td>1551157200</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/au1bdu/conditions_of_entry...</td>\n",
       "      <td>t2_nguj2</td>\n",
       "      <td>eh6cdqn</td>\n",
       "      <td>au1bdu</td>\n",
       "      <td>Reddit__Herring</td>\n",
       "      <td>t3_au1bdu</td>\n",
       "      <td>That was really awesome. Very interesting conc...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_d64uh2</td>\n",
       "      <td>1568955600</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/d64uh2/the_188minute_man/f...</td>\n",
       "      <td>t2_2vpmh2gy</td>\n",
       "      <td>f0rojyz</td>\n",
       "      <td>d64uh2</td>\n",
       "      <td>jcammarato</td>\n",
       "      <td>t1_f0ridzm</td>\n",
       "      <td>Yes, but she also has no choice and will event...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169583</th>\n",
       "      <td>t3_b8zif8</td>\n",
       "      <td>1554440400</td>\n",
       "      <td>7</td>\n",
       "      <td>/r/nosleep/comments/b8zif8/the_lynch_house/ek2...</td>\n",
       "      <td>t2_20igrc4i</td>\n",
       "      <td>ek27hib</td>\n",
       "      <td>b8zif8</td>\n",
       "      <td>BlondeRR1717</td>\n",
       "      <td>t3_b8zif8</td>\n",
       "      <td>Are you aware that you wrote natural causes fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169584</th>\n",
       "      <td>t3_bgj62b</td>\n",
       "      <td>1556168400</td>\n",
       "      <td>73</td>\n",
       "      <td>/r/nosleep/comments/bgj62b/my_first_breath_too...</td>\n",
       "      <td>t2_lg9ark7</td>\n",
       "      <td>ellzl0a</td>\n",
       "      <td>bgj62b</td>\n",
       "      <td>Bismuthie</td>\n",
       "      <td>t1_ellx4dp</td>\n",
       "      <td>Omg smart</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169585</th>\n",
       "      <td>t3_btonzl</td>\n",
       "      <td>1559106000</td>\n",
       "      <td>2</td>\n",
       "      <td>/r/nosleep/comments/btonzl/her_eye_was_a_spira...</td>\n",
       "      <td>t2_gkau4</td>\n",
       "      <td>ep15bo3</td>\n",
       "      <td>btonzl</td>\n",
       "      <td>thejollyden</td>\n",
       "      <td>t1_ep12y0d</td>\n",
       "      <td>How do you delete comments?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169586</th>\n",
       "      <td>t3_cn3nbl</td>\n",
       "      <td>1565326800</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/cn3nbl/straight_to_vhs_sun...</td>\n",
       "      <td>t2_17gt7w</td>\n",
       "      <td>ew7ctpd</td>\n",
       "      <td>cn3nbl</td>\n",
       "      <td>LadyGrey1174</td>\n",
       "      <td>t3_cn3nbl</td>\n",
       "      <td>Holy hannah, time for a Disney movie...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169587</th>\n",
       "      <td>t3_dh11jg</td>\n",
       "      <td>1571202000</td>\n",
       "      <td>1</td>\n",
       "      <td>/r/nosleep/comments/dh11jg/my_friend_and_i_fou...</td>\n",
       "      <td>t2_21i87yt2</td>\n",
       "      <td>f3oyfms</td>\n",
       "      <td>dh11jg</td>\n",
       "      <td>TheCorrectAyhZad</td>\n",
       "      <td>t1_f3ju9rf</td>\n",
       "      <td>A cult dedicated to breeding Homo Sapiens and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169588 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          link_id     sortKey  score  \\\n",
       "0       t3_bs22s7  1558760400     10   \n",
       "1       t3_dukiqw  1573621200      1   \n",
       "2       t3_e4lcwk  1575349200      4   \n",
       "3       t3_au1bdu  1551157200      1   \n",
       "4       t3_d64uh2  1568955600      1   \n",
       "...           ...         ...    ...   \n",
       "169583  t3_b8zif8  1554440400      7   \n",
       "169584  t3_bgj62b  1556168400     73   \n",
       "169585  t3_btonzl  1559106000      2   \n",
       "169586  t3_cn3nbl  1565326800      1   \n",
       "169587  t3_dh11jg  1571202000      1   \n",
       "\n",
       "                                                permalink author_fullname  \\\n",
       "0       /r/nosleep/comments/bs22s7/i_work_on_a_boat_ou...        t2_wcuxx   \n",
       "1       /r/nosleep/comments/dukiqw/a_childs_method_for...        t2_86jlh   \n",
       "2       /r/nosleep/comments/e4lcwk/everyone_knows_the_...        t2_j5mx0   \n",
       "3       /r/nosleep/comments/au1bdu/conditions_of_entry...        t2_nguj2   \n",
       "4       /r/nosleep/comments/d64uh2/the_188minute_man/f...     t2_2vpmh2gy   \n",
       "...                                                   ...             ...   \n",
       "169583  /r/nosleep/comments/b8zif8/the_lynch_house/ek2...     t2_20igrc4i   \n",
       "169584  /r/nosleep/comments/bgj62b/my_first_breath_too...      t2_lg9ark7   \n",
       "169585  /r/nosleep/comments/btonzl/her_eye_was_a_spira...        t2_gkau4   \n",
       "169586  /r/nosleep/comments/cn3nbl/straight_to_vhs_sun...       t2_17gt7w   \n",
       "169587  /r/nosleep/comments/dh11jg/my_friend_and_i_fou...     t2_21i87yt2   \n",
       "\n",
       "             id storyId            author   parent_id  \\\n",
       "0       eok7qb9  bs22s7        Wolf_of_WV   t3_bs22s7   \n",
       "1       f77papf  dukiqw          Sporkazm  t1_f77p7a1   \n",
       "2       f9erm02  e4lcwk           LiKenun   t3_e4lcwk   \n",
       "3       eh6cdqn  au1bdu   Reddit__Herring   t3_au1bdu   \n",
       "4       f0rojyz  d64uh2        jcammarato  t1_f0ridzm   \n",
       "...         ...     ...               ...         ...   \n",
       "169583  ek27hib  b8zif8      BlondeRR1717   t3_b8zif8   \n",
       "169584  ellzl0a  bgj62b         Bismuthie  t1_ellx4dp   \n",
       "169585  ep15bo3  btonzl       thejollyden  t1_ep12y0d   \n",
       "169586  ew7ctpd  cn3nbl      LadyGrey1174   t3_cn3nbl   \n",
       "169587  f3oyfms  dh11jg  TheCorrectAyhZad  t1_f3ju9rf   \n",
       "\n",
       "                                                     body  sentiment  \\\n",
       "0       You are a dead man walking.  The people who ar...        0.0   \n",
       "1       &amp;#x200B;\\n\\nSomehow I came to return the e...        0.0   \n",
       "2       &gt;My neck was sticky, and stank, stank  like...        1.0   \n",
       "3       That was really awesome. Very interesting conc...        1.0   \n",
       "4       Yes, but she also has no choice and will event...        NaN   \n",
       "...                                                   ...        ...   \n",
       "169583  Are you aware that you wrote natural causes fo...        NaN   \n",
       "169584                                          Omg smart        NaN   \n",
       "169585                        How do you delete comments?        NaN   \n",
       "169586            Holy hannah, time for a Disney movie...        NaN   \n",
       "169587  A cult dedicated to breeding Homo Sapiens and ...        NaN   \n",
       "\n",
       "        prediction  \n",
       "0               -1  \n",
       "1               -1  \n",
       "2               -1  \n",
       "3               -1  \n",
       "4               -1  \n",
       "...            ...  \n",
       "169583          -1  \n",
       "169584          -1  \n",
       "169585          -1  \n",
       "169586          -1  \n",
       "169587          -1  \n",
       "\n",
       "[169588 rows x 12 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df\n",
    "#comment_df.to_csv (r'user_comments.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Astayanax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Astayanax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Astayanax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Astayanax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The punkt module is a pre-trained model that helps you tokenize words and sentences.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "positive_comments = comment_df[comment_df['sentiment'] == 1]\n",
    "negative_comments = comment_df[comment_df['sentiment'] == 0]\n",
    "\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for count in range(len(positive_comments)):\n",
    "    positive_tokens.append(tknzr.tokenize(positive_comments.iloc[count][\"body\"]))\n",
    "    \n",
    "for count in range(len(negative_comments)):\n",
    "    negative_tokens.append(tknzr.tokenize(negative_comments.iloc[count][\"body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(comment_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(comment_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "negative_tokens_c = []\n",
    "positive_tokens_c = []\n",
    "\n",
    "for count in range(len(negative_tokens)):\n",
    "    negative_tokens_c.append(remove_noise(negative_tokens[count], stop_words))\n",
    "    \n",
    "for count in range(len(positive_tokens)):\n",
    "    positive_tokens_c.append(remove_noise(positive_tokens[count], stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('’', 27), ('get', 17), ('good', 14), ('like', 12), ('read', 12), ('kill', 11), ('think', 11), ('go', 10), ('op', 10), (\"i'm\", 10)]\n",
      "[('like', 15), ('r', 15), ('nosleep', 15), ('#x200b', 14), ('get', 14), ('question', 14), ('must', 14), ('message', 14), ('moderator', 14), ('submission', 13)]\n"
     ]
    }
   ],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "            \n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tokens)\n",
    "\n",
    "pos_tokens_mod = get_tweets_for_model(positive_tokens_c)\n",
    "neg_tokens_mod = get_tweets_for_model(negative_tokens_c)\n",
    "\n",
    "all_pos_words = get_all_words(positive_tokens_c)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "all_neg_words = get_all_words(negative_tokens_c)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(freq_dist_pos.most_common(10))\n",
    "print(freq_dist_neg.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 89 196\n",
      "137 59\n"
     ]
    }
   ],
   "source": [
    "#NB - You CAN'T run this multiple times in a row. It basically removes the data from the\n",
    "# *_tokens_mod variables. If you need to rerun this, please rerun the code above first\n",
    "# to repopulate these variables\n",
    "positive_dataset = [(comment_dict, \"Positive\")\n",
    "                     for comment_dict in pos_tokens_mod]\n",
    "\n",
    "negative_dataset = [(comment_dict, \"Negative\")\n",
    "                     for comment_dict in neg_tokens_mod]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "train_size = int(len(dataset)*0.7)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:train_size]\n",
    "test_data = dataset[train_size:]\n",
    "\n",
    "print(len(positive_dataset), len(negative_dataset), len(dataset))\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.576271186440678\n",
      "Most Informative Features\n",
      "                 comment = True           Negati : Positi =      7.2 : 1.0\n",
      "                    post = True           Negati : Positi =      7.2 : 1.0\n",
      "               subreddit = True           Negati : Positi =      7.2 : 1.0\n",
      "                       r = True           Negati : Positi =      6.4 : 1.0\n",
      "                   check = True           Negati : Positi =      6.4 : 1.0\n",
      "                    must = True           Negati : Positi =      6.4 : 1.0\n",
      "                   issue = True           Negati : Positi =      5.5 : 1.0\n",
      "                     may = True           Negati : Positi =      3.3 : 1.0\n",
      "                      op = True           Positi : Negati =      3.3 : 1.0\n",
      "                    good = True           Positi : Negati =      3.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Yes, but she also has no choice and will eventually crave a companion as well.\n",
      "Positive I'm glad your husband decided to confront her. You would have never known otherwise.\n",
      "Negative Thank you. I don’t know what I believe anymore, but I appreciate your words.\n",
      "Positive Beautiful!\n",
      "Negative I did in a comment,but they've allbeen removed by nosleep.\n",
      "\n",
      "I copy/pasted here for you:\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "The first was [removed for believability](https://www.reddit.com/r/nosleep/comments/c2kbxo/warningyouarenotsafe/). I didn't see anything that I thought it should be removed for. The second one is [here.](https://www.reddit.com/r/nosleep/comments/c2l80b/did_you_get_the_alert/)\n",
      "\n",
      "[This](https://www.reddit.com/r/nosleep/comments/c1wxxu/i_work_at_nasa_we_made_alien_contact_yesterday/) was posted two days ago...not saying it's related, not saying it isn't.\n",
      "\n",
      "But I'm scared.\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "EDIT. Oh, holy shit. All alert posts have been removed. WTF.\n",
      "Positive Happens with far more stories on this sub than it should. I don’t know if people get bored telling their true stories and rush the ending, or if the endings are just really hard to convey. Either way it’s disappointing every time it happens\n",
      "Negative r/hydrohomies must be in on it\n",
      "Negative I did not realize I was reading /r/nosleep and I thought this was an IAMA for a second\n",
      "Positive So wholesome. Love it.\n",
      "Negative No comment.\n"
     ]
    }
   ],
   "source": [
    "random_comments = comment_df[comment_df['sentiment'] == -1]\n",
    "random_comments\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "for count in range(10):\n",
    "    custom_tokens = remove_noise(tknzr.tokenize(random_comments.iloc[count][\"body\"]))\n",
    "    predicted_sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "    \n",
    "    print(predicted_sentiment, random_comments.iloc[count][\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
